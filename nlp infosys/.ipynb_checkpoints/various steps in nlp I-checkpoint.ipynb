{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d042d1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cafdd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6c98d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'fundamentals']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"I am reading NLP fundamentals\")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f058e74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('fundamentals', 'NNS')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d40781a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec9102a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words  = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a8cae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I am learning Python. It is one of the most popular programming languages.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed70c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b76703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python', '.', 'It', 'is', 'one', 'of', 'the', 'most', 'popular', 'programming', 'languages', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49f626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_no_stops = ' '.join([word for word in sentence_words if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78780e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python . It one popular programming languages .\n"
     ]
    }
   ],
   "source": [
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b77e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I visited US from UK on 27-10-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "638076a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalised_sentence = sentence.replace('US' , \"United States\").replace(\"UK\" ,\"United Kingdom\").replace(\"-18\" , \"-2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1ca171c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United States from United Kingdom on 27-10-2018\n"
     ]
    }
   ],
   "source": [
    "print(normalised_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9232c9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "     ------ ------------------------------- 112.6/622.8 kB 6.4 MB/s eta 0:00:01\n",
      "     -------------------- ----------------- 337.9/622.8 kB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 622.8/622.8 kB 4.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622374 sha256=ff2afffc68e3ba1e1d3a88b11fe2ec7ab4d95ef676d67df7e36076acf8b6b991\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\5e\\90\\99\\807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16f97ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6136342b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell(\"Natureal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50db58b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = word_tokenize(\"Ntural Lyanguage Protcessing deqals with the art of extracting imsights from Natural Language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8cb9849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ntural', 'Lyanguage', 'Protcessing', 'deqals', 'with', 'the', 'art', 'of', 'extracting', 'imsights', 'from', 'Natural', 'Language', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7a182e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated,             use autocorrect.Speller instead\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected = ' '.join([spell(word) for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f3688a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing deals with the art of extracting insights from Natural Language .\n"
     ]
    }
   ],
   "source": [
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e2eca",
   "metadata": {},
   "source": [
    "STEMMING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edadb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcf7b46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87f25bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d96cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fire'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"firing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39149ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battl'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"battling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed5f88",
   "metadata": {},
   "source": [
    "LEMMATIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9cbae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c232233",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16841b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf47cfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'production'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"production\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc60b130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coming'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"coming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b85c2311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'battle'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"battle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14824f",
   "metadata": {},
   "source": [
    "NAMED ENTITY RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a050d669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8ad12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d270ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9d83a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2321f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Packt', 'NNP')]), Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)) , binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e930431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('We', 'PRP'), 2],\n",
       " [('are', 'VBP'), 2],\n",
       " [('reading', 'VBG'), 2],\n",
       " [('a', 'DT'), 2],\n",
       " [('book', 'NN'), 2],\n",
       " [('published', 'VBN'), 2],\n",
       " [('by', 'IN'), 2],\n",
       " [Tree('NE', [('Packt', 'NNP')]), 1],\n",
       " [('which', 'WDT'), 2],\n",
       " [('is', 'VBZ'), 2],\n",
       " [('based', 'VBN'), 2],\n",
       " [('out', 'IN'), 2],\n",
       " [('of', 'IN'), 2],\n",
       " [Tree('NE', [('Birmingham', 'NNP')]), 1],\n",
       " [('.', '.'), 2]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[a , len(a)] for a in i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e29cb",
   "metadata": {},
   "source": [
    "WORD DISAMBIGUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d078cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a97183",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"Keep your savings in the bank\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a682c678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n"
     ]
    }
   ],
   "source": [
    "print(lesk(word_tokenize(sentence1) , 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a18db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"It's so risky to drive over the banks of the road\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93702365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "print(lesk(word_tokenize(sentence2) , 'bank'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d36fb16",
   "metadata": {},
   "source": [
    "SENTENCE BOUNDARY DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4239b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98837f4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We are reading a book.',\n",
       " 'Do you kow who is the publisher?',\n",
       " 'It is Packt.',\n",
       " 'Packt is based out of Birminghm.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"We are reading a book.\\\n",
    "               Do you kow who is the publisher?\\\n",
    "               It is Packt. Packt is based out of Birminghm.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12460088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
